# üéì Graduation Project: Advanced Sensor Data Analysis & Machine Learning Platform

## üìã Project Overview

**Graduation Project** is a comprehensive, enterprise-grade sensor data analysis and machine learning platform developed as part of the Kaitech company training program. This project represents the culmination of advanced Python programming skills, combining data science, machine learning, and modern software development practices.

### üåü **What This Project Does**

This application is a **complete sensor data analysis ecosystem** that processes, analyzes, and predicts outcomes from industrial sensor readings. It handles temperature, stress, and displacement data from multiple sensors, providing:

- **Real-time Data Processing**: Convert raw sensor data into structured formats
- **Advanced Analytics**: Statistical analysis, trend identification, and anomaly detection
- **Machine Learning Models**: Predictive maintenance, structural health prediction, and anomaly detection
- **Multiple Interfaces**: Console, GUI, and API access methods
- **Data Export**: Excel and JSON export capabilities
- **User Management**: Secure login system with profile management

### üéØ **Target Applications**

- **Industrial Monitoring**: Factory equipment health monitoring
- **Predictive Maintenance**: Prevent equipment failures before they happen
- **Quality Control**: Monitor production line sensor data
- **Research & Development**: Data analysis for engineering projects
- **Educational Purposes**: Learning data science and machine learning concepts

## üöÄ **Key Features & Capabilities**

### üî¨ **Core Data Analysis Functions**

#### 1. **Data Structure Conversion**
- Convert sensor data to NumPy arrays with proper data types
- Parse timestamps and ensure data integrity
- Handle multiple sensor types and data formats

#### 2. **Statistical Analysis**
- Calculate averages per sensor for all metrics
- Identify extreme values (maximum/minimum temperatures)
- Generate comprehensive statistical summaries
- Compare readings across different time intervals

#### 3. **Data Organization & Grouping**
- Organize readings by sensor ID
- Group data using advanced dictionary structures
- Extract unique sensor identifiers
- Create time-based data hierarchies

#### 4. **Advanced Data Processing**
- Filter data based on temperature thresholds
- Sort and deduplicate timestamps
- Extract most recent readings per sensor
- Handle missing or incomplete data

### ü§ñ **Machine Learning Capabilities**

#### 1. **Predictive Maintenance Model**
- **Algorithm**: Decision Tree Classifier
- **Purpose**: Predict equipment failures based on sensor readings
- **Features**: Temperature, stress, and displacement data
- **Output**: Binary classification (failure/normal operation)
- **Accuracy**: Includes comprehensive model evaluation metrics

#### 2. **Anomaly Detection System**
- **Algorithm**: Isolation Forest
- **Purpose**: Identify unusual sensor readings that may indicate problems
- **Features**: Multi-dimensional sensor data analysis
- **Output**: Anomaly scores and flagged readings
- **Applications**: Quality control and early warning systems

#### 3. **Structural Health Prediction**
- **Algorithm**: Random Forest Classifier
- **Purpose**: Assess structural integrity based on sensor data
- **Features**: Multi-class classification (healthy, moderate, critical)
- **Thresholds**: Configurable stress and displacement limits
- **Output**: Health status predictions with confidence scores

### üñ•Ô∏è **Multiple User Interfaces**

#### 1. **Console Interface**
- **Graduation Project Console**: Full-featured analysis menu
- **Task 8 Console**: NumPy-based data processing
- **Task 7 Console**: Pandas-based data analysis
- **Main Console**: Comprehensive sensor data analysis

#### 2. **Graphical User Interface (GUI)**
- **Modern PyQt6 Interface**: Professional-grade desktop application
- **User Authentication**: Secure login and registration system
- **Dashboard**: Intuitive button-based navigation
- **Results Display**: Real-time data visualization
- **Profile Management**: User settings and preferences

#### 3. **Data Export Interfaces**
- **Excel Export**: Multi-sheet Excel files with organized data
- **JSON Export**: Structured data for API integration
- **Flexible Formats**: Choose export type or export to both

## üõ†Ô∏è **Installation & Setup**

### üìã **System Requirements**

#### **Minimum Requirements**
- **Operating System**: Windows 10/11, macOS 10.14+, or Ubuntu 18.04+
- **Python Version**: Python 3.8 or higher
- **RAM**: 4 GB minimum (8 GB recommended)
- **Storage**: 2 GB available disk space
- **Display**: 1024x768 minimum resolution

#### **Recommended Requirements**
- **Operating System**: Windows 11, macOS 12+, or Ubuntu 20.04+
- **Python Version**: Python 3.9 or higher
- **RAM**: 8 GB or higher
- **Storage**: 5 GB available disk space
- **Display**: 1920x1080 or higher resolution

### üîß **Installation Methods**

#### **Method 1: Automated Installation (Recommended)**

1. **Clone or Download the Project**
   ```bash
   # If using Git
   git clone <repository-url>
   cd Graduation_project
   
   # Or download and extract the ZIP file
   # Navigate to the extracted folder
   ```

2. **Install Dependencies**
   ```bash
   # Install all required packages
   pip install -r requirements.txt
   
   # Or install manually if requirements.txt is not available
   pip install PyQt6 pandas numpy matplotlib scikit-learn openpyxl
   ```

3. **Verify Installation**
   ```bash
   # Test Python imports
   python -c "import PyQt6, pandas, numpy, matplotlib, sklearn; print('All packages installed successfully!')"
   ```

#### **Method 2: Manual Package Installation**

If you prefer to install packages individually:

```bash
# Core GUI framework
pip install PyQt6

# Data analysis libraries
pip install pandas numpy matplotlib

# Machine learning libraries
pip install scikit-learn

# Excel file handling
pip install openpyxl

# Additional utilities
pip install datetime json os sys
```

#### **Method 3: Conda Installation (Alternative)**

```bash
# Create new conda environment
conda create -n sensor_analysis python=3.9

# Activate environment
conda activate sensor_analysis

# Install packages
conda install -c conda-forge pyqt pandas numpy matplotlib scikit-learn openpyxl
```

### üìÅ **File Structure Setup**

Ensure your project folder contains these essential files:

```
Graduation_project/
‚îú‚îÄ‚îÄ Final_project.py          # Main application file
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ users.txt                # User credentials (auto-created)
‚îú‚îÄ‚îÄ logo_5e37c4cd90a4c.png  # Company logo
‚îú‚îÄ‚îÄ sensor_readings.xlsx     # Data export file (auto-created)
‚îî‚îÄ‚îÄ README.md               # This documentation file
```

### üîê **Initial Setup**

1. **First Run Setup**
   ```bash
   # Run the application for the first time
   python Final_project.py
   ```

2. **Create User Account**
   - The application will start with the Graduation Project console
   - For GUI access, uncomment the `main_gui()` line in the code
   - Create your first user account through the signup interface

3. **Verify Data Files**
   - Check that `sensor_readings.xlsx` is created automatically
   - Ensure `users.txt` contains your login credentials

## üöÄ **Running the Application**

### **Method 1: Graduation Project Console (Default)**

```bash
python Final_project.py
```

**Features Available:**
- Convert sensor data to NumPy arrays
- Calculate averages per sensor
- Identify highest stress sensors
- Extract high-temperature readings
- Plot stress over time
- Run predictive maintenance models
- Detect anomalies
- Structural health prediction

### **Method 2: GUI Application**

1. **Edit the main section in `Final_project.py`:**
   ```python
   if __name__ == "__main__":
       # Comment out: Grduation_Project_console()
       main_gui()  # Uncomment this line
   ```

2. **Run the application:**
   ```bash
   python Final_project.py
   ```

3. **GUI Features:**
   - Professional login/signup system
   - Interactive dashboard with all functions
   - Real-time results display
   - Data export options (Excel/JSON)
   - User profile management

### **Method 3: Task-Specific Consoles**

#### **Task 8 Console (NumPy Focus)**
```python
if __name__ == "__main__":
    task_8_console()  # Uncomment this line
```

#### **Task 7 Console (Pandas Focus)**
```python
if __name__ == "__main__":
    task_7_console()  # Uncomment this line
```

#### **Main Console (Comprehensive Analysis)**
```python
if __name__ == "__main__":
    main_console()  # Uncomment this line
```

## üìä **Data Structure & Format**

### **Sensor Data Format**

The application processes sensor data in this structure:

```python
sensor_data = [
    ("S1", "2025-04-28 10:00", 35.2, 12.1, 0.002),  # (ID, Timestamp, Temp, Stress, Displacement)
    ("S2", "2025-04-28 10:00", 36.5, 14.0, 0.003),
    ("S1", "2025-04-28 11:00", 36.1, 12.5, 0.0021),
    # ... more readings
]
```

### **Data Types**

- **SensorID**: String identifier (e.g., "S1", "S2", "S3")
- **Timestamp**: ISO format datetime string
- **Temperature**: Float in Celsius (¬∞C)
- **Stress**: Float value (engineering units)
- **Displacement**: Float value in meters (m)

### **Data Processing Pipeline**

1. **Raw Data Input** ‚Üí List of tuples
2. **Data Validation** ‚Üí Type checking and format validation
3. **Structure Conversion** ‚Üí NumPy arrays or Pandas DataFrames
4. **Analysis Processing** ‚Üí Statistical calculations and ML models
5. **Results Generation** ‚Üí Formatted output and visualizations
6. **Data Export** ‚Üí Excel/JSON files

## üîç **Function Reference**

### **Core Data Functions**

#### **`convert_sensor_data_to_numpy_array_G(sensor_data)`**
- **Purpose**: Convert sensor data to structured NumPy array
- **Input**: List of sensor data tuples
- **Output**: NumPy array with proper data types
- **Use Case**: Data preprocessing for analysis

#### **`calculate_average_per_sensor_G(sensor_data)`**
- **Purpose**: Calculate average metrics per sensor
- **Input**: Sensor data array
- **Output**: Dictionary with sensor averages
- **Metrics**: Temperature, stress, displacement averages

#### **`identify_sensor_with_highest_average_stress_G(sensor_data)`**
- **Purpose**: Find sensor with highest stress readings
- **Input**: Sensor data array
- **Output**: Tuple of (sensor_id, average_stress)
- **Application**: Equipment stress monitoring

#### **`extract_readings_where_temperature_more_than_36_G(sensor_data)`**
- **Purpose**: Filter high-temperature readings
- **Input**: Sensor data array
- **Threshold**: 36.0¬∞C
- **Output**: Filtered NumPy array
- **Use Case**: Overheating detection

### **Machine Learning Functions**

#### **`train_predictive_maintenance_model(sensor_data)`**
- **Algorithm**: Decision Tree Classifier
- **Purpose**: Predict equipment failures
- **Features**: Temperature, stress, displacement
- **Output**: Trained model with evaluation metrics
- **Training**: 70% training, 30% testing split

#### **`detect_anomalies(sensor_data)`**
- **Algorithm**: Isolation Forest
- **Purpose**: Identify unusual sensor readings
- **Contamination**: 20% (configurable)
- **Output**: Anomaly labels (-1 for anomalies)
- **Application**: Quality control and monitoring

#### **`train_structural_health_prediction_model(sensor_data)`**
- **Algorithm**: Random Forest Classifier
- **Purpose**: Assess structural integrity
- **Classes**: 0 (healthy), 1 (moderate), 2 (critical)
- **Thresholds**: Configurable stress and displacement limits
- **Output**: Health status predictions

### **Data Organization Functions**

#### **`organize_readings(sensor_data)`**
- **Purpose**: Group readings by sensor ID
- **Input**: Raw sensor data
- **Output**: Dictionary with sensor-organized data
- **Structure**: `{sensor_id: [readings_list]}`

#### **`Extreme_sensors(data)`**
- **Purpose**: Find sensors with extreme temperature values
- **Input**: Organized sensor data
- **Output**: List of extreme sensor information
- **Metrics**: Maximum and minimum temperatures

#### **`Compare_readings(data)`**
- **Purpose**: Compare readings across time intervals
- **Input**: Organized sensor data
- **Output**: Temperature change analysis
- **Analysis**: Increase, decrease, or constant patterns

#### **`Summarize_data(data, choice)`**
- **Purpose**: Generate statistical summaries
- **Input**: Organized data and metric choice
- **Choices**: Temperature (1), Stress (2), Displacement (3), All (4)
- **Output**: Max, min, and average statistics

### **Data Export Functions**

#### **Excel Export Functions**
- **`Organize_readings_per_sensor_save_to_excel(data)`**
- **`Extreme_sensors_save_to_excel(data)`**
- **`Compare_readings_save_to_excel(data)`**
- **`Summarize_data_save_to_excel(data)`**
- **`Group_data_by_sensor_save_to_excel(data)`**

#### **JSON Export Function**
- **`write_to_json(data, json_file_name)`**

## üé® **User Interface Guide**

### **Console Interface Navigation**

#### **Graduation Project Console Menu**
```
1. Convert the sensor data into a structured NumPy array
2. Compute average temperature, stress, and displacement per sensor
3. Identify the sensor with the highest average stress
4. Extract all readings where temperature > 36.0¬∞C
5. Plot stress over time per sensor
6. Run Predictive Maintenance Model
7. Detect anomalies
8. Run Structural Health Prediction Model
9. Exit
```

#### **Main Console Menu**
```
1. Organize readings per sensor
2. Identify unique sensors with extreme values
3. Compare readings from different time intervals
4. Summarize the data by calculating max, min, and average readings per sensor
5. Group data by sensor using a dictionary
6. Use sets to find unique sensor IDs that recorded stress > 13.0
7. Calculate statistics per sensor
8. Max, min, and average temperature and max displacement
9. Extract all timestamps into a list and sort them
10. Create a tuple of the most recent reading for each sensor
11. Exit
```

### **GUI Interface Features**

#### **Login System**
- **User Registration**: Create new accounts with email verification
- **Secure Login**: Password-protected access
- **Profile Management**: Update user information and passwords
- **Session Management**: Secure logout functionality

#### **Dashboard Interface**
- **Function Buttons**: Easy access to all analysis functions
- **Results Display**: Real-time output in scrollable text area
- **Export Options**: Choose between Excel, JSON, or both formats
- **Navigation**: Intuitive button-based interface

#### **Settings & Profile**
- **Profile Updates**: Modify name, email, and password
- **Security**: Current password verification for changes
- **Data Persistence**: Automatic user data saving

## üìà **Data Visualization**

### **Available Charts**

#### **1. Stress Over Time Plot**
- **Function**: `plot_stress_over_time(sensor_data)`
- **Type**: Line chart with markers
- **Features**: Multiple sensors, different colors, grid lines
- **Use Case**: Trend analysis and pattern recognition

#### **2. Temperature vs Time Plot**
- **Function**: `plot_temperature_vs_time(sensor_data)`
- **Type**: Line chart with markers
- **Features**: Sensor-specific colors, legend, grid
- **Use Case**: Temperature monitoring and trend analysis

#### **3. Stress vs Displacement Scatter Plot**
- **Function**: `plot_stress_vs_displacement(sensor_data)`
- **Type**: Scatter plot
- **Features**: Sensor grouping, legend, grid
- **Use Case**: Correlation analysis between metrics

### **Chart Customization**

All charts include:
- **Professional Styling**: Clean, readable design
- **Automatic Scaling**: Dynamic axis adjustment
- **Legend Support**: Clear sensor identification
- **Grid Lines**: Easy value reading
- **Export Capability**: Save charts as images

## üîß **Configuration & Customization**

### **Modifying Sensor Data**

To use your own sensor data:

```python
# Replace the default sensor_data with your own
sensor_data = [
    ("YOUR_SENSOR_1", "2025-01-01 00:00", 25.0, 10.0, 0.001),
    ("YOUR_SENSOR_2", "2025-01-01 00:00", 26.0, 11.0, 0.002),
    # Add more readings as needed
]
```

### **Adjusting Machine Learning Parameters**

#### **Predictive Maintenance Model**
```python
# Modify contamination factor for anomaly detection
model = IsolationForest(contamination=0.1, random_state=42)  # 10% contamination

# Adjust test split ratio
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

#### **Structural Health Thresholds**
```python
# Customize health status thresholds
if row["Stress"] > 15.0 or row["Displacement"] > 0.004:  # Adjust these values
    conditions.append(2)  # Critical condition
elif row["Stress"] > 13.0 or row["Displacement"] > 0.003:  # Adjust these values
    conditions.append(1)  # Moderate condition
else:
    conditions.append(0)  # Healthy condition
```

### **GUI Customization**

#### **Styling Changes**
```python
# Modify color scheme
self.setStyleSheet("background-color: #your_color;")

# Change button colors
btn.setStyleSheet("""
    QPushButton {
        background-color: #your_color;
        color: white;
        padding: 15px;
        border: none;
        border-radius: 10px;
    }
""")
```

#### **Layout Modifications**
```python
# Adjust window size
self.setGeometry(100, 100, 1400, 900)  # Width, Height

# Modify spacing
layout.setSpacing(25)  # Increase button spacing
```

## üß™ **Testing & Validation**

### **Built-in Test Cases**

The application includes sample data for testing:

```python
# Test data structure
sensor_data = [
    ("S1", "2025-04-28 10:00", 35.2, 12.1, 0.002),
    ("S2", "2025-04-28 10:00", 36.5, 14.0, 0.003),
    ("S1", "2025-04-28 11:00", 36.1, 12.5, 0.0021),
    ("S3", "2025-04-28 10:00", 34.0, 11.8, 0.0025),
    ("S2", "2025-04-28 11:00", 37.2, 14.3, 0.0031),
    ("S1", "2025-04-28 12:00", 37.0, 13.0, 0.0022),
]
```

### **Validation Methods**

#### **Data Integrity Checks**
- **Type Validation**: Ensures correct data types
- **Range Validation**: Checks for reasonable sensor values
- **Timestamp Validation**: Verifies date format consistency

#### **Model Performance Metrics**
- **Classification Report**: Precision, recall, F1-score
- **Confusion Matrix**: True/false positive/negative rates
- **Accuracy Metrics**: Overall model performance

### **Testing Workflow**

1. **Run Basic Functions**: Test data conversion and analysis
2. **Verify Calculations**: Check statistical accuracy
3. **Test ML Models**: Validate prediction quality
4. **Export Testing**: Verify file generation
5. **GUI Testing**: Test all interface elements

## üö® **Troubleshooting Guide**

### **Common Issues & Solutions**

#### **1. Import Errors**

**Problem**: `ModuleNotFoundError: No module named 'PyQt6'`
```bash
# Solution: Install PyQt6
pip install PyQt6

# Alternative: Use conda
conda install -c conda-forge pyqt
```

**Problem**: `ModuleNotFoundError: No module named 'sklearn'`
```bash
# Solution: Install scikit-learn
pip install scikit-learn

# Alternative: Use conda
conda install scikit-learn
```

#### **2. GUI Display Issues**

**Problem**: GUI doesn't appear or crashes
```python
# Solution: Check PyQt6 installation
python -c "from PyQt6.QtWidgets import QApplication; print('PyQt6 working')"

# Alternative: Use different backend
export QT_QPA_PLATFORM=xcb  # Linux
export QT_QPA_PLATFORM=windows  # Windows
```

**Problem**: Logo image not displaying
```python
# Solution: Check file path and format
# Ensure logo_5e37c4cd90a4c.png exists in the same directory
# Verify image format (PNG, JPG supported)
```

#### **3. Data Processing Errors**

**Problem**: `ValueError: could not convert string to float`
```python
# Solution: Check data format
# Ensure all numeric values are properly formatted
# Verify timestamp format is consistent
```

**Problem**: `IndexError: list index out of range`
```python
# Solution: Check data structure
# Ensure sensor_data list is not empty
# Verify all tuples have 5 elements
```

#### **4. Machine Learning Errors**

**Problem**: `ValueError: Input contains NaN`
```python
# Solution: Clean data before ML processing
# Remove or fill missing values
# Ensure all features are numeric
```

**Problem**: `ValueError: Number of labels does not match number of samples`
```python
# Solution: Check data and label alignment
# Ensure X and y have same number of rows
# Verify label format consistency
```

### **Debug Mode**

Enable debug output by adding print statements:

```python
# Add debug prints to any function
def your_function(data):
    print(f"Debug: Input data: {data}")
    print(f"Debug: Data type: {type(data)}")
    print(f"Debug: Data length: {len(data)}")
    
    # Your processing code here
    
    print(f"Debug: Output: {result}")
    return result
```

### **Performance Optimization**

#### **Large Dataset Handling**
```python
# For large datasets, use chunking
def process_large_data(data, chunk_size=1000):
    results = []
    for i in range(0, len(data), chunk_size):
        chunk = data[i:i+chunk_size]
        chunk_result = process_chunk(chunk)
        results.extend(chunk_result)
    return results
```

#### **Memory Management**
```python
# Clear variables when not needed
import gc

def memory_intensive_function(data):
    # Process data
    result = heavy_processing(data)
    
    # Clear memory
    del data
    gc.collect()
    
    return result
```

## üîÑ **Extending the Application**

### **Adding New Analysis Functions**

#### **Example: Correlation Analysis**
```python
def analyze_correlations(sensor_data):
    """
    Analyze correlations between different sensor metrics.
    """
    df = convert_sensor_data_to_dataframe(sensor_data)
    
    # Calculate correlation matrix
    correlation_matrix = df[["Temperature", "Stress", "Displacement"]].corr()
    
    # Create heatmap
    plt.figure(figsize=(8, 6))
    plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto')
    plt.colorbar()
    plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)
    plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)
    plt.title("Sensor Metrics Correlation Matrix")
    
    # Add correlation values
    for i in range(len(correlation_matrix.columns)):
        for j in range(len(correlation_matrix.columns)):
            plt.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', 
                    ha='center', va='center')
    
    plt.tight_layout()
    plt.show()
    
    return correlation_matrix
```

#### **Example: Time Series Analysis**
```python
def analyze_trends(sensor_data):
    """
    Analyze time-based trends in sensor data.
    """
    df = convert_sensor_data_to_dataframe(sensor_data)
    
    # Group by sensor and calculate rolling averages
    for sensor_id in df["SensorID"].unique():
        sensor_data = df[df["SensorID"] == sensor_id]
        
        # Calculate rolling average
        rolling_temp = sensor_data["Temperature"].rolling(window=2).mean()
        rolling_stress = sensor_data["Stress"].rolling(window=2).mean()
        
        # Plot trends
        plt.figure(figsize=(10, 6))
        plt.subplot(2, 1, 1)
        plt.plot(sensor_data["Timestamp"], rolling_temp, label=f"Sensor {sensor_id}")
        plt.title(f"Temperature Trends - Sensor {sensor_id}")
        plt.ylabel("Temperature (¬∞C)")
        plt.legend()
        plt.grid(True)
        
        plt.subplot(2, 1, 2)
        plt.plot(sensor_data["Timestamp"], rolling_stress, label=f"Sensor {sensor_id}")
        plt.title(f"Stress Trends - Sensor {sensor_id}")
        plt.ylabel("Stress")
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        plt.show()
```

### **Adding New Machine Learning Models**

#### **Example: Support Vector Machine**
```python
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

def train_svm_model(sensor_data):
    """
    Train Support Vector Machine for classification.
    """
    parsed_data = convert_sensor_data_to_numpy_array_G(sensor_data)
    df = pd.DataFrame(parsed_data)
    
    # Prepare features and labels
    X = df[["Temperature", "Stress", "Displacement"]]
    y = [0, 1, 1, 0, 1, 1]  # Your labels
    
    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state=42
    )
    
    # Train SVM
    svm_model = SVC(kernel='rbf', random_state=42)
    svm_model.fit(X_train, y_train)
    
    # Evaluate
    predictions = svm_model.predict(X_test)
    print("SVM Model Report:")
    print(classification_report(y_test, predictions))
    
    return svm_model, scaler
```

### **Adding New Export Formats**

#### **Example: CSV Export**
```python
def export_to_csv(data, filename):
    """
    Export data to CSV format.
    """
    import csv
    
    with open(filename, 'w', newline='') as csvfile:
        if isinstance(data, dict):
            # Handle dictionary data
            writer = csv.writer(csvfile)
            for key, value in data.items():
                writer.writerow([key, value])
        elif isinstance(data, list):
            # Handle list data
            writer = csv.writer(csvfile)
            for item in data:
                writer.writerow([item])
        else:
            # Handle other data types
            writer = csv.writer(csvfile)
            writer.writerow([data])
    
    print(f"Data exported to {filename}")
```

## üìö **Learning Resources**

### **Python Programming Concepts**

#### **Object-Oriented Programming**
- **Classes and Objects**: Understanding class structure
- **Inheritance**: Extending existing functionality
- **Encapsulation**: Data hiding and access control
- **Polymorphism**: Different behaviors for different types

#### **Data Structures**
- **Lists and Tuples**: Ordered data collections
- **Dictionaries**: Key-value data organization
- **Sets**: Unique element collections
- **NumPy Arrays**: Multi-dimensional numerical data

#### **File Handling**
- **File I/O**: Reading and writing files
- **JSON Processing**: Structured data serialization
- **Excel Integration**: Spreadsheet data handling
- **Error Handling**: Try-catch exception management

### **Data Science Concepts**

#### **Data Analysis**
- **Statistical Measures**: Mean, median, standard deviation
- **Data Filtering**: Conditional data selection
- **Data Grouping**: Organizing data by categories
- **Time Series**: Temporal data analysis

#### **Machine Learning**
- **Supervised Learning**: Classification and regression
- **Unsupervised Learning**: Clustering and anomaly detection
- **Feature Engineering**: Data preparation for ML
- **Model Evaluation**: Performance metrics and validation

#### **Data Visualization**
- **Chart Types**: Line, scatter, bar charts
- **Plot Customization**: Colors, labels, legends
- **Interactive Plots**: Dynamic visualizations
- **Export Options**: Saving charts as images

### **Software Development Concepts**

#### **User Interface Design**
- **GUI Frameworks**: PyQt6 application development
- **Layout Management**: Organizing interface elements
- **Event Handling**: User interaction processing
- **Responsive Design**: Adapting to different screen sizes

#### **Code Organization**
- **Modular Design**: Separating functionality into functions
- **Code Reusability**: Writing maintainable code
- **Documentation**: Clear code comments and docstrings
- **Error Handling**: Graceful failure management

## ü§ù **Contributing & Development**

### **Code Style Guidelines**

#### **Python PEP 8 Standards**
```python
# Use descriptive names
def calculate_average_temperature(sensor_readings):
    """Calculate average temperature from sensor readings."""
    pass

# Add proper docstrings
def process_sensor_data(data):
    """
    Process raw sensor data into structured format.
    
    Args:
        data (list): Raw sensor data as list of tuples
        
    Returns:
        dict: Processed data organized by sensor ID
        
    Raises:
        ValueError: If data format is invalid
    """
    pass

# Use consistent formatting
if temperature > threshold:
    return "high"
else:
    return "normal"
```

#### **Function Design Principles**
- **Single Responsibility**: Each function does one thing well
- **Input Validation**: Check inputs before processing
- **Error Handling**: Provide meaningful error messages
- **Return Values**: Consistent return format

### **Testing New Features**

#### **Development Workflow**
1. **Plan Feature**: Define requirements and design
2. **Implement Code**: Write the functionality
3. **Test Locally**: Verify with sample data
4. **Integration Test**: Ensure compatibility with existing code
5. **Documentation**: Update README and code comments

#### **Testing Checklist**
- [ ] Function works with sample data
- [ ] Handles edge cases (empty data, invalid inputs)
- [ ] Integrates with existing functions
- [ ] Export functionality works correctly
- [ ] GUI integration (if applicable)
- [ ] Error messages are helpful

### **Suggested Improvements**

#### **Short-term Enhancements**
- **Data Validation**: More robust input checking
- **Error Logging**: Comprehensive error tracking
- **Performance Optimization**: Faster data processing
- **Additional Charts**: More visualization options

#### **Long-term Features**
- **Database Integration**: Persistent data storage
- **Real-time Monitoring**: Live sensor data processing
- **API Development**: RESTful service interface
- **Mobile App**: Cross-platform mobile interface
- **Cloud Deployment**: Web-based application

## üìÑ **License & Legal Information**

### **Project License**

This project is part of the **Kaitech company training program**. Please respect the company's intellectual property rights and usage guidelines.

### **Usage Restrictions**

- **Educational Use**: Free for learning and educational purposes
- **Commercial Use**: Requires company permission
- **Distribution**: Do not redistribute without authorization
- **Modification**: Modifications for personal use are allowed

### **Attribution Requirements**

When using or referencing this project:

```
Author: Tarek Adel Ali
Company: Kaitech
Project: Graduation Project - Sensor Data Analysis
Date: 2025-07-21
Email: tarekadel314@gmail.com
```

## üë®‚Äçüíª **Author Information**

### **Developer Profile**

- **Name**: Tarek Adel Ali
- **Company**: Kaitech
- **Position**: Training Program Participant
- **Date**: 2025-07-21
- **Email**: tarekadel314@gmail.com
- **Project**: Task 08 - Advanced Sensor Data Analysis

### **Technical Skills Demonstrated**

- **Python Programming**: Advanced Python development
- **Data Science**: Statistical analysis and machine learning
- **GUI Development**: Professional PyQt6 applications
- **Data Processing**: NumPy, Pandas, and data manipulation
- **Machine Learning**: Scikit-learn model development
- **Software Architecture**: Modular, maintainable code design

### **Project Achievements**

- **Comprehensive Solution**: Complete sensor analysis platform
- **Multiple Interfaces**: Console, GUI, and export options
- **Advanced Analytics**: ML models and statistical analysis
- **Professional Quality**: Enterprise-grade application
- **Educational Value**: Demonstrates advanced programming concepts

## üîó **Related Projects**

### **Task Series Overview**

This project builds upon and integrates concepts from:

- **Task 02**: Linear equation solver with OOP
- **Task 03**: Electrical circuit analysis with OOP
- **Task 07**: Pandas-based sensor data analysis
- **Task 08**: NumPy-based sensor data analysis

### **Technology Stack Integration**

- **NumPy**: Numerical computing and array operations
- **Pandas**: Data manipulation and analysis
- **Matplotlib**: Data visualization and plotting
- **Scikit-learn**: Machine learning algorithms
- **PyQt6**: Modern GUI development
- **OpenPyXL**: Excel file handling

## üìû **Support & Contact**

### **Getting Help**

#### **Self-Help Resources**
1. **Check this README**: Comprehensive documentation
2. **Review Code Comments**: Inline explanations
3. **Test with Sample Data**: Verify functionality
4. **Check Error Messages**: Look for specific error details

#### **Common Questions**

**Q: How do I add my own sensor data?**
A: Replace the `sensor_data` list with your own data following the same format.

**Q: Can I modify the machine learning parameters?**
A: Yes, all ML models have configurable parameters in their respective functions.

**Q: How do I export data to different formats?**
A: Use the built-in export functions or modify them to support additional formats.

**Q: Can I run this without the GUI?**
A: Yes, the console interfaces provide full functionality without GUI dependencies.

### **Contact Information**

For questions, issues, or contributions:

- **Email**: tarekadel314@gmail.com
- **Project**: Graduation Project - Kaitech Training
- **Response Time**: Within 24-48 hours
- **Support Scope**: Code explanation, usage guidance, troubleshooting

### **Issue Reporting**

When reporting issues, please include:

1. **Error Message**: Complete error text
2. **Python Version**: `python --version`
3. **Operating System**: Windows/Mac/Linux version
4. **Steps to Reproduce**: Detailed reproduction steps
5. **Expected vs Actual Behavior**: What you expected vs what happened

## üéâ **Conclusion**

This **Graduation Project** represents a comprehensive solution for sensor data analysis, combining advanced programming concepts with practical applications. The project demonstrates:

- **Professional Development Skills**: Enterprise-grade code quality
- **Advanced Data Science**: Machine learning and statistical analysis
- **Modern Software Design**: Multiple interfaces and export options
- **Educational Value**: Clear examples of programming concepts
- **Practical Applications**: Real-world sensor monitoring solutions

Whether you're learning Python programming, exploring data science, or implementing sensor monitoring systems, this project provides a solid foundation and comprehensive examples to build upon.

---

**Happy Coding and Data Analysis! üöÄüìä**

*This README provides complete documentation for understanding, installing, using, and extending the Graduation Project sensor data analysis platform. For additional support or questions, please contact the author at tarekadel314@gmail.com.*
